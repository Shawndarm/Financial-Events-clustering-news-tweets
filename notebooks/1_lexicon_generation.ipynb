{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcdb5c2",
   "metadata": {},
   "source": [
    "# Lexicon Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee78fd78",
   "metadata": {},
   "source": [
    "The objective is to identify relevant words for finacial markets. \n",
    "To achieve this, \n",
    "\n",
    "\n",
    "\n",
    "- Objectif : Identifier les mots qui ont un impact réel sur le marché financier.\n",
    "- Procédure : \n",
    "    - Prendre les articles de presse sur une fenêtre de 4 semaines précédant le jour $d$.\n",
    "    - Calculer la corrélation de chaque mot avec la variation du prix de l'indice ($\\Delta$) le jour suivant sa publication.\n",
    "    - Filtrage des mots trop fréquents (>90% des documents) ou trop rares (<10 documents).\n",
    "    - Sélection des mots situés dans les percentiles extrêmes (les plus positifs et les plus négatifs) pour former le lexique \"conscient du temps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "916c2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from gensim.parsing import preprocessing as pproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afb69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1695e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958983a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78760e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a93095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60372b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c289f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69320396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Chargement et nettoyage des news (patience...)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Index(['headline'], dtype='str')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_12796\\1766316804.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    111\u001b[39m news_path = \u001b[33m'../data/processed/news_2020_2025.csv'\u001b[39m\n\u001b[32m    112\u001b[39m price_path = \u001b[33m'../data/raw/sp500_prices.csv'\u001b[39m\n\u001b[32m    113\u001b[39m \n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# --- EXECUTION ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m df_final = generate_full_period_lexicons(news_path, price_path)\n\u001b[32m    116\u001b[39m print(\u001b[33m\"Fichier '../data/processed/lexique_quotidien_2020_2025.csv' généré avec succès !\"\u001b[39m)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_12796\\1766316804.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(news_path, price_path)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m generate_full_period_lexicons(news_path, price_path):\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# A. Chargement des News\u001b[39;00m\n\u001b[32m     35\u001b[39m     print(\u001b[33m\"1. Chargement et nettoyage des news (patience...)\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     df_news = pd.read_csv(news_path).drop_duplicates(subset=[\u001b[33m'headline'\u001b[39m])\n\u001b[32m     37\u001b[39m     df_news[\u001b[33m'date_dt'\u001b[39m] = pd.to_datetime(df_news[\u001b[33m'date_gdelt'\u001b[39m].astype(str).str[:\u001b[32m8\u001b[39m], format=\u001b[33m'%Y%m%d'\u001b[39m).dt.date\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# On nettoie tout une seule fois ici\u001b[39;00m\n\u001b[32m     39\u001b[39m     tqdm.pandas()\n",
      "\u001b[32mc:\\Users\\dutau\\Desktop\\Finance Quantitative\\Projet_Roland_Maeva\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, subset, keep, inplace, ignore_index)\u001b[39m\n\u001b[32m   7937\u001b[39m \n\u001b[32m   7938\u001b[39m         inplace = validate_bool_kwarg(inplace, \u001b[33m\"inplace\"\u001b[39m)\n\u001b[32m   7939\u001b[39m         ignore_index = validate_bool_kwarg(ignore_index, \u001b[33m\"ignore_index\"\u001b[39m)\n\u001b[32m   7940\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7941\u001b[39m         result = self[-self.duplicated(subset, keep=keep)]\n\u001b[32m   7942\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[32m   7943\u001b[39m             result.index = default_index(len(result))\n\u001b[32m   7944\u001b[39m \n",
      "\u001b[32mc:\\Users\\dutau\\Desktop\\Finance Quantitative\\Projet_Roland_Maeva\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, subset, keep)\u001b[39m\n\u001b[32m   8067\u001b[39m         \u001b[38;5;66;03m# Otherwise, raise a KeyError, same as if you try to __getitem__ with a\u001b[39;00m\n\u001b[32m   8068\u001b[39m         \u001b[38;5;66;03m# key that doesn't exist.\u001b[39;00m\n\u001b[32m   8069\u001b[39m         diff = set(subset) - set(self.columns)\n\u001b[32m   8070\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[32m-> \u001b[39m\u001b[32m8071\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(Index(diff))\n\u001b[32m   8072\u001b[39m \n\u001b[32m   8073\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(subset) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m self.columns.is_unique:\n\u001b[32m   8074\u001b[39m             \u001b[38;5;66;03m# GH#45236 This is faster than get_group_index below\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: Index(['headline'], dtype='str')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 1. NETTOYAGE GLOBAL (VITESSE OPTIMISÉE) ---\n",
    "def clean_text_authors(text):\n",
    "    if not isinstance(text, str) or len(text) < 20:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    # Retrait abréviations\n",
    "    text = re.sub(r'(?:[a-z]\\.)+', lambda m: m.group().replace('.', ''), text)\n",
    "    # Gensim strip punctuation & stopwords\n",
    "    text = pproc.strip_punctuation(text)\n",
    "    text = pproc.remove_stopwords(text)\n",
    "    # Filtre alphabétique strict (retrait du bruit numérique '05pm', etc.)\n",
    "    tokens = [w for w in text.split() if w.isalpha() and len(w) > 2]\n",
    "    # Stemming\n",
    "    return \" \".join([pproc.stem_text(w) for w in tokens])\n",
    "\n",
    "# --- 2. FONCTION DE RECHERCHE DU DELTA (WEEK-ENDS) ---\n",
    "def get_next_delta(pub_date, market_dict, max_days=4):\n",
    "    for i in range(1, max_days + 1):\n",
    "        next_d = pub_date + timedelta(days=i)\n",
    "        if next_d in market_dict:\n",
    "            return market_dict[next_d]\n",
    "    return None\n",
    "\n",
    "# --- 3. GÉNÉRATION DU LEXIQUE POUR TOUTE LA PÉRIODE ---\n",
    "def generate_full_period_lexicons(news_path, price_path):\n",
    "    # A. Chargement des News\n",
    "    print(\"1. Chargement et nettoyage des news (patience...)\")\n",
    "    df_news = pd.read_csv(news_path).drop_duplicates(subset=['headline'])\n",
    "    df_news['date_dt'] = pd.to_datetime(df_news['date_gdelt'].astype(str).str[:8], format='%Y%m%d').dt.date\n",
    "    # On nettoie tout une seule fois ici\n",
    "    tqdm.pandas()\n",
    "    df_news['clean_text'] = (df_news['headline'].fillna('') + \" \" + df_news['body'].fillna('')).progress_apply(clean_text_authors)\n",
    "\n",
    "    # B. Chargement des Prix\n",
    "    print(\"2. Chargement des prix et calcul des Delta...\")\n",
    "    df_price = pd.read_csv(price_path, skiprows=3, names=['Date', 'Close', 'High', 'Low', 'Open', 'Volume'])\n",
    "    df_price['Date'] = pd.to_datetime(df_price['Date']).dt.date\n",
    "    df_price = df_price.dropna(subset=['Close']).sort_values('Date')\n",
    "    df_price['Delta'] = df_price['Close'].pct_change()\n",
    "    market_dict = df_price.set_index('Date')['Delta'].to_dict()\n",
    "\n",
    "    # C. Définition de la période (Départ le 1er Février 2019 pour avoir 28j d'historique)\n",
    "    start_date = datetime(2019, 2, 1).date()\n",
    "    end_date = df_price['Date'].max()\n",
    "    \n",
    "    all_daily_lexicons = [] # On va stocker ça en liste pour faire un CSV à la fin\n",
    "\n",
    "    print(f\"3. Calcul des lexiques quotidiens de {start_date} à {end_date}...\")\n",
    "    pbar = tqdm(total=(end_date - start_date).days + 1)\n",
    "    \n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        # Fenêtre glissante [D-28, D-1]\n",
    "        window_news = df_news[(df_news['date_dt'] >= (current_date - timedelta(days=28))) & \n",
    "                              (df_news['date_dt'] < current_date)].copy()\n",
    "        \n",
    "        if len(window_news) >= 10:\n",
    "            # Attribution du delta impact d+1\n",
    "            window_news['y'] = window_news['date_dt'].apply(lambda d: get_next_delta(d, market_dict))\n",
    "            window_news = window_news.dropna(subset=['y'])\n",
    "            \n",
    "            if not window_news.empty:\n",
    "                # TF-IDF Binaire\n",
    "                vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, binary=True, use_idf=False, norm=None)\n",
    "                try:\n",
    "                    X = vectorizer.fit_transform(window_news['clean_text'])\n",
    "                    words = vectorizer.get_feature_names_out()\n",
    "                    \n",
    "                    # Multiplication par Delta\n",
    "                    deltas = window_news['y'].values\n",
    "                    for i in range(X.shape[0]):\n",
    "                        X.data[X.indptr[i]:X.indptr[i+1]] *= deltas[i]\n",
    "                    \n",
    "                    # Score f(j)\n",
    "                    counts = np.array(X.getnnz(axis=0))\n",
    "                    counts[counts == 0] = 1\n",
    "                    scores = np.asarray(X.sum(axis=0)).flatten() / counts\n",
    "                    \n",
    "                    # Screening Percentiles 80/20\n",
    "                    p80, p20 = np.percentile(scores, 80), np.percentile(scores, 20)\n",
    "                    \n",
    "                    pos_words = [words[i] for i in range(len(words)) if scores[i] >= p80 and scores[i] > 0]\n",
    "                    neg_words = [words[i] for i in range(len(words)) if scores[i] <= p20 and scores[i] < 0]\n",
    "                    \n",
    "                    # Sauvegarde pour ce jour\n",
    "                    all_daily_lexicons.append({\n",
    "                        'date': current_date,\n",
    "                        'positive_words': \",\".join(pos_words),\n",
    "                        'negative_words': \",\".join(neg_words)\n",
    "                    })\n",
    "                except: pass\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # D. Sauvegarde finale\n",
    "    lexicon_df = pd.DataFrame(all_daily_lexicons)\n",
    "    lexicon_df.to_csv('lexique_quotidien_2019_2023.csv', index=False)\n",
    "    return lexicon_df\n",
    "\n",
    "news_path = '../data/processed/news_2019_2023_cleaned.csv'\n",
    "price_path = '../data/raw/sp500_prices.csv'\n",
    "\n",
    "# --- EXECUTION ---\n",
    "df_final = generate_full_period_lexicons('news_2019_2023.csv', 'sp500_prices.csv')\n",
    "print(\"Fichier 'lexique_quotidien_2019_2023.csv' généré avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0e9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a13b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01085ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0c0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bdecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcac114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf5fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c44826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb33991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790f324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet_Roland_Maeva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
