# Event Detection in Finance by Clustering News and Tweets ðŸ“ˆðŸ“°

![Python Badge](https://img.shields.io/badge/Python-3.10%2B-blue)
![Data Science Badge](https://img.shields.io/badge/Data_Science-NLP_%7C_Clustering-orange)
![Academic Badge](https://img.shields.io/badge/UniversitÃ©_Paris_1-PanthÃ©on_Sorbonne-maroon)

**Authors:** Roland DUTAUZIET & Maeva N'GUESSAN

**Program:** Master 2 MOSEF - Data Science (ModÃ©lisations Statistiques Ã‰conomiques et FinanciÃ¨res) - 2025/2026  

**Context:** Quantitative Finance Project

## ðŸ“– Project Overview

This project is a reproduction and an extension of the academic paper *"Event Detection in Finance by Clustering News and Tweets"* (Carta et al., 2021). The core objective is to build a robust NLP pipeline capable of detecting major financial events by clustering professional news articles, and then validating these events by measuring the "Social Heat" (public attention) through social media platforms (Twitter/Stocktwits).

We applied this methodology to the **S&P 500 index for the year 2023**, a period marked by high volatility, including the Silicon Valley Bank (SVB) collapse and and the ARM IPO.



---

## ðŸ—ï¸ Project Architecture

The repository is structured to ensure reproducibility and clean code separation.

```text
Financial-Events-clustering-news-tweets
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ for_models/
â”‚   â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”‚   â”œâ”€â”€ table_3_tweet_assignment_AI.csv
â”‚   â”‚   â”‚   â”‚   # Tweet-to-cluster assignment results for the AI enthusiasm period (Mayâ€“July 2023).
â”‚   â”‚   â”‚   â”‚   # Used for quantitative evaluation and representative tweet analysis.
â”‚   â”‚   â”‚   â”œâ”€â”€ table_3_tweet_assignment_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Tweet assignment results for the Silicon Valley Bank crisis period (March 2023).
â”‚   â”‚   â”‚   â”‚   # Contains cosine similarity scores and assigned event IDs.
â”‚   â”‚   â”‚   â”œâ”€â”€ clean_news_week_AI.csv
â”‚   â”‚   â”‚   â”‚   # Preprocessed weekly news dataset for the AI event window.
â”‚   â”‚   â”‚   â”œâ”€â”€ clean_news_week_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Preprocessed weekly news dataset for the SVB crisis window.
â”‚   â”‚   â”‚   â”œâ”€â”€ final_event_signatures_AI.csv
â”‚   â”‚   â”‚   â”‚   # Median-based centroids (event signatures) after outlier removal (AI period).
â”‚   â”‚   â”‚   â”œâ”€â”€ final_event_signatures_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Robust cluster centroids after cleaning (SVB period).
â”‚   â”‚   â”‚   â”œâ”€â”€ news_features.csv
â”‚   â”‚   â”‚   â”‚   # 300-dimensional document embeddings (GloVe) for each news article.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned.csv
â”‚   â”‚   â”‚   â”‚   # Full tweet assignment output across all periods.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned_AI.csv
â”‚   â”‚   â”‚   â”‚   # Tweets assigned to clusters during AI enthusiasm period.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Tweets assigned to clusters during SVB crisis.
â”‚   â”‚   â”‚   â””â”€â”€ tweets_features.csv
â”‚   â”‚   â”‚       # 300-dimensional embeddings for tweets (same GloVe model as news).
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”‚       â”œâ”€â”€ daily_dtm/
â”‚   â”‚       â”‚   # Daily binary Document-Term Matrices used for Marginal Screening.
â”‚   â”‚       â”œâ”€â”€ daily_lexicons_filtered/
â”‚   â”‚       â”‚   # Daily filtered lexicons (P20/P80 percentile selection).
â”‚   â”‚       â”œâ”€â”€ daily_lexicons_full/
â”‚   â”‚       â”‚   # Full lexicons before percentile filtering.
â”‚   â”‚       â”œâ”€â”€ news_2023.csv
â”‚   â”‚       â”‚   # 1,565 financial news articles (GDELT, Yahoo Finance, CNBC).
â”‚   â”‚       â”œâ”€â”€ news_2023_clean.csv
â”‚   â”‚       â”‚   # Cleaned and preprocessed version of news_2023.csv.
â”‚   â”‚       â”œâ”€â”€ sp500_2023.csv
â”‚   â”‚       â”‚   # Daily S&P 500 prices (271 trading days). Used for return computation and ground truth.
â”‚   â”‚       â””â”€â”€ tweets_2023.csv
â”‚   â”‚           # 2,243 filtered financial tweets mentioning $SPX, $SPY, or S&P 500.
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ P10 Event detection in finance using ...
â”‚   â”‚   # Original reference paper (Carta et al., 2021).
â”‚   â”œâ”€â”€ Rapport_Event_detection_Roland_...
â”‚   â”‚   # Full academic report (methodology, results, evaluation).
â”‚   â””â”€â”€ Slides Finance quantitative.pdf
â”‚       # Presentation slides summarizing the project.
â”‚
â”œâ”€â”€ img/
â”‚   â”œâ”€â”€ 1_lexicon_generation/
â”‚   â”‚   # Marginal Screening plots and percentile threshold visualizations.
â”‚   â”œâ”€â”€ 2_feature_engineering/
â”‚   â”‚   # Embedding illustrations and vector representations.
â”‚   â”œâ”€â”€ 3_news_clustering/
â”‚   â”‚   # Silhouette scores, dendrograms, t-SNE visualizations.
â”‚   â”œâ”€â”€ 4_outlier_removal/
â”‚   â”‚   # Cluster cleaning and centroid repositioning figures.
â”‚   â”œâ”€â”€ 5_relevant_words_extraction/
â”‚   â”‚   # TF-IDF keyword extraction results per cluster.
â”‚   â”œâ”€â”€ 6_tweets_assignment/
â”‚   â”‚   # Tweet-to-cluster similarity visualizations.
â”‚   â”œâ”€â”€ 7_alert_generation/
â”‚   â”‚   # Alert ratio plots and ground truth comparisons.
â”‚   â”œâ”€â”€ 6_tweets_assignment.zip
â”‚   â”‚   # Archived visual results for tweet assignment.
â”‚   â””â”€â”€ 7_alert_generation.zip
â”‚       # Archived visual results for alert generation.
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 0_extract_data.ipynb
â”‚   â”‚   # Data ingestion, preprocessing, sentiment computation (VADER).
â”‚   â”œâ”€â”€ 1_lexicon_generation.ipynb
â”‚   â”‚   # Implementation of Marginal Screening and daily lexicon construction.
â”‚   â”œâ”€â”€ 2_feature_engineering.ipynb
â”‚   â”‚   # GloVe embedding computation and document vector construction.
â”‚   â”œâ”€â”€ 3_news_clustering.ipynb
â”‚   â”‚   # Hierarchical clustering (HAC), silhouette maximization, centroid computation.
â”‚   â”œâ”€â”€ 4_outlier_removal.ipynb
â”‚   â”‚   # Double-criterion outlier removal (silhouette + cosine similarity).
â”‚   â”œâ”€â”€ 5_relevant_words_extraction.ipynb
â”‚   â”‚   # TF-IDF keyword extraction per cluster.
â”‚   â”œâ”€â”€ 6_tweet_assignment.ipynb
â”‚   â”‚   # Tweet embedding and cosine similarity-based assignment.
â”‚   â””â”€â”€ 7_Alert_generation.ipynb
â”‚       # Alert computation, ground truth construction, Precision/Recall/F-score evaluation.
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   # Compiled Python bytecode (ignored by git).
â”‚   â”œâ”€â”€ alert_generation.py
â”‚   â”‚   # Computes daily assignment ratio R(d), generates alerts (R(d) > Î¸),
â”‚   â”‚   # builds S&P 500 ground truth (|weekly return| > 2%), and evaluates Precision/Recall/F-score.
â”‚   â”œâ”€â”€ extract_data.py
â”‚   â”‚   # Handles dataset loading, cleaning, date alignment,
â”‚   â”‚   # tweet filtering by cashtags, and sentiment scoring (VADER).
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   # Filters articles using daily lexicons and computes 300D GloVe embeddings.
â”‚   â”œâ”€â”€ lexicon_generation.py
â”‚   â”‚   # Builds daily binary DTM, computes Marginal Screening f(j),
â”‚   â”‚   # selects positive/negative financial terms via percentile thresholds.
â”‚   â”œâ”€â”€ news_clustering.py
â”‚   â”‚   # Runs Agglomerative Clustering (cosine + average linkage),
â”‚   â”‚   # compares with K-Means/K-Medians, computes silhouette scores,
â”‚   â”‚   # and calculates median-based centroids (event signatures).
â”‚   â”œâ”€â”€ outlier_removal.py
â”‚   â”‚   # Applies double filtering: per-sample silhouette + centroid cosine similarity.
â”‚   â”‚   # Removes noisy articles and recalculates centroids.
â”‚   â”œâ”€â”€ relevant_words_extraction.py
â”‚   â”‚   # Computes TF-IDF within each cluster to extract top representative financial terms.
â”‚   â””â”€â”€ tweet_assignment.py
â”‚       # Embeds tweets using the same GloVe model,
â”‚       # assigns them to nearest event centroids via cosine similarity (threshold = 0.5).
â”‚
â”œâ”€â”€ .gitignore
â”‚   # Excludes large data files, virtual environments, and cache folders.
â”‚
â”œâ”€â”€ .python-version
â”‚   # Specifies Python interpreter version for reproducibility.
â”‚
â”œâ”€â”€ LICENSE
â”‚   # Project license.
â”‚
â”œâ”€â”€ README.md
â”‚   # Project documentation and methodological overview.
â”‚
â”œâ”€â”€ pyproject.toml
â”‚   # Project metadata and dependency management (uv-compatible).
â”‚
â””â”€â”€ uv.lock
    # Locked dependency versions ensuring reproducible environments.
```

---

## Installation & Setup

We use uv, an extremely fast Python package and project manager written in Rust, to handle our virtual environment and dependencies.

### 1. Install uv
If you haven't installed uv yet, run:

```Bash
# On Windows (PowerShell)
pip install uv
# On macOS/Linux
curl -LsSf [https://astral.sh/uv/install.sh](https://astral.sh/uv/install.sh) | sh
```
### 2. Setup the Virtual Environment
Navigate to the project directory, create and activate the environment:

```Bash
cd Financial-Events-clustering-news-tweets
uv venv
# Windows:
 .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate
```
3. Install Dependencies
Install the required packages (Pandas, Numpy, Scikit-learn, Plotly, SciPy, Gensim):

```Bash
uv sync
```

# Financial Event Detection using News & Tweets (S&P 500 â€“ 2023)

## Abstract

This project reproduces and extends the methodology of Carta et al. (2021) for detecting financial events by clustering news articles and measuring their resonance on social media.  

Applied to the S&P 500 in 2023, the pipeline combines dynamic lexicon generation, document embeddings, hierarchical clustering, and tweet-based alert generation.  

The objective is to automatically identify significant market events and evaluate detection performance against real S&P 500 price movements.

---

# Methodology & Pipeline

Our framework follows a 7-step architecture inspired by the original paper, adapted to the 2023 market environment.

---

# Step 1 â€” Lexicon Generation

We build a **dynamic financial lexicon** using Marginal Screening on a rolling window of news articles.

Each word receives a score:

$$
f(j) = \frac{1}{N} \sum_{k=1}^{N} X_k^{(j)} \cdot \delta_k
$$

Words strongly correlated with market returns (top/bottom percentiles) are retained, while neutral words are discarded.

ðŸ“Œ *Insert Screenshot:* Lexicon distribution or word cloud.

---

# Step 2 â€” Feature Engineering

Each article is converted into a dense vector representation.

- Text is cleaned and filtered using the daily lexicon.
- Pre-trained 300D word embeddings (GloVe) are used.
- The document vector is computed as:

$$
v_a = \frac{1}{|W_a|} \sum_{w \in W_a} \text{Embedding}(w)
$$

This transforms textual information into numerical form for clustering.

---

# Step 3 â€” News Clustering

News embeddings are grouped into candidate events.

We tested several algorithms (K-Means, HAC, etc.) and selected the optimal number of clusters via **Silhouette Score maximization**:

$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$

Hierarchical Agglomerative Clustering (cosine distance + average linkage) consistently produced the most coherent clusters.

ðŸ“Œ *Insert Screenshot:* Silhouette plot or t-SNE visualization.

---

# Step 4 â€” Relevant Words & Cleaning

To ensure interpretability:

- We extract top financial terms per cluster using TF-IDF.
- Outliers are removed using silhouette and centroid similarity thresholds.

Only semantically coherent financial clusters are retained.

ðŸ“Œ *Insert Screenshot:* Top keywords per cluster.

---

# Step 5 â€” Event Signatures

For each validated cluster, we compute a centroid (event signature):

$$
c_k = \text{median}(\{v_a\})
$$

This vector summarizes the detected event and serves as a reference for social media matching.

---

# Step 6 â€” Tweet Assignment

Tweets are embedded using the same model and compared to event signatures using cosine similarity:

$$
\text{sim}(t, c_k) = \frac{t \cdot c_k}{\|t\| \|c_k\|}
$$

Tweets with similarity above a threshold are assigned to the event.

ðŸ“Œ *Insert Screenshot:* Tweet assignment distribution.

---

# Step 7 â€” Alert Generation & Evaluation

We define **Social Heat** as:

$$
R(d) = \frac{\text{Assigned Tweets}_d}{\text{Total Tweets}_d}
$$

An alert is triggered when:

$$
R(d) > \theta
$$

### Ground Truth

Market event intervals are defined using weekly S&P 500 variation:

$$
\Delta_d = \frac{|close(d+7) - close(d)|}{close(d)}
$$

Days where:

$$
\Delta_d > 0.02
$$

are labeled as event periods.

We evaluate performance using **Precision, Recall, and F-Score**.

ðŸ“Œ *Insert Screenshot:* S&P 500 price with ground truth intervals and social alerts.

---

# Case Studies (2023)

### Silicon Valley Bank Collapse (March 2023)
Detected strong social amplification prior to major market drawdown.

### ARM IPO (September 2023)
Identified anticipation and immediate post-listing reaction.

---

# Key Findings

- **HAC Dominance:** Hierarchical clustering outperformed K-Means and density-based methods.
- **Recall-Oriented Detection:** Capturing major market moves is prioritized over minimizing false alarms.
- **Social Heat Dynamics:** Social activity sometimes anticipates or slightly lags price movements, suggesting potential alpha signals.

---

# Acknowledgments

**Original Paper:**  
Carta, S., et al. (2021). *Event Detection in Finance by Clustering News and Tweets.*

**Institution:**  
UniversitÃ© Paris 1 PanthÃ©on-Sorbonne â€” Master 2 MOSEF (Quantitative Finance)
