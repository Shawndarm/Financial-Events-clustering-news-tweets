# Event Detection in Finance by Clustering News and Tweets ðŸ“ˆðŸ“°

![Python Badge](https://img.shields.io/badge/Python-3.10%2B-blue)
![Data Science Badge](https://img.shields.io/badge/Data_Science-NLP_%7C_Clustering-orange)
![Academic Badge](https://img.shields.io/badge/UniversitÃ©_Paris_1-PanthÃ©on_Sorbonne-maroon)

**Authors:** Roland DUTAUZIET & Maeva N'GUESSAN
**Program:** Master 2 MOSEF - Data Science (ModÃ©lisations Statistiques Ã‰conomiques et FinanciÃ¨res) - 2025/2026  
**Context:** Quantitative Finance Project

## ðŸ“– Project Overview

This project is a reproduction and an extension of the academic paper *"Event Detection in Finance by Clustering News and Tweets"* (Carta et al., 2021). The core objective is to build a robust NLP pipeline capable of detecting major financial events by clustering professional news articles, and then validating these events by measuring the "Social Heat" (public attention) through social media platforms (Twitter/Stocktwits).

We applied this methodology to the **S&P 500 index for the year 2023**, a period marked by high volatility, including the Silicon Valley Bank (SVB) collapse and and the ARM IPO.



---

## ðŸ—ï¸ Project Architecture

The repository is structured to ensure reproducibility and clean code separation.

```text
Financial-Events-clustering-news-tweets
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ for_models/
â”‚   â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”‚   â”œâ”€â”€ table_3_tweet_assignment_AI.csv
â”‚   â”‚   â”‚   â”‚   # Tweet-to-cluster assignment results for the AI enthusiasm period (Mayâ€“July 2023).
â”‚   â”‚   â”‚   â”‚   # Used for quantitative evaluation and representative tweet analysis.
â”‚   â”‚   â”‚   â”œâ”€â”€ table_3_tweet_assignment_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Tweet assignment results for the Silicon Valley Bank crisis period (March 2023).
â”‚   â”‚   â”‚   â”‚   # Contains cosine similarity scores and assigned event IDs.
â”‚   â”‚   â”‚   â”œâ”€â”€ clean_news_week_AI.csv
â”‚   â”‚   â”‚   â”‚   # Preprocessed weekly news dataset for the AI event window.
â”‚   â”‚   â”‚   â”œâ”€â”€ clean_news_week_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Preprocessed weekly news dataset for the SVB crisis window.
â”‚   â”‚   â”‚   â”œâ”€â”€ final_event_signatures_AI.csv
â”‚   â”‚   â”‚   â”‚   # Median-based centroids (event signatures) after outlier removal (AI period).
â”‚   â”‚   â”‚   â”œâ”€â”€ final_event_signatures_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Robust cluster centroids after cleaning (SVB period).
â”‚   â”‚   â”‚   â”œâ”€â”€ news_features.csv
â”‚   â”‚   â”‚   â”‚   # 300-dimensional document embeddings (GloVe) for each news article.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned.csv
â”‚   â”‚   â”‚   â”‚   # Full tweet assignment output across all periods.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned_AI.csv
â”‚   â”‚   â”‚   â”‚   # Tweets assigned to clusters during AI enthusiasm period.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Tweets assigned to clusters during SVB crisis.
â”‚   â”‚   â”‚   â””â”€â”€ tweets_features.csv
â”‚   â”‚   â”‚       # 300-dimensional embeddings for tweets (same GloVe model as news).
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”‚       â”œâ”€â”€ daily_dtm/
â”‚   â”‚       â”‚   # Daily binary Document-Term Matrices used for Marginal Screening.
â”‚   â”‚       â”œâ”€â”€ daily_lexicons_filtered/
â”‚   â”‚       â”‚   # Daily filtered lexicons (P20/P80 percentile selection).
â”‚   â”‚       â”œâ”€â”€ daily_lexicons_full/
â”‚   â”‚       â”‚   # Full lexicons before percentile filtering.
â”‚   â”‚       â”œâ”€â”€ news_2023.csv
â”‚   â”‚       â”‚   # 1,565 financial news articles (GDELT, Yahoo Finance, CNBC).
â”‚   â”‚       â”œâ”€â”€ news_2023_clean.csv
â”‚   â”‚       â”‚   # Cleaned and preprocessed version of news_2023.csv.
â”‚   â”‚       â”œâ”€â”€ sp500_2023.csv
â”‚   â”‚       â”‚   # Daily S&P 500 prices (271 trading days). Used for return computation and ground truth.
â”‚   â”‚       â””â”€â”€ tweets_2023.csv
â”‚   â”‚           # 2,243 filtered financial tweets mentioning $SPX, $SPY, or S&P 500.
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ P10 Event detection in finance using ...
â”‚   â”‚   # Original reference paper (Carta et al., 2021).
â”‚   â”œâ”€â”€ Rapport_Event_detection_Roland_...
â”‚   â”‚   # Full academic report (methodology, results, evaluation).
â”‚   â””â”€â”€ Slides Finance quantitative.pdf
â”‚       # Presentation slides summarizing the project.
â”‚
â”œâ”€â”€ img/
â”‚   â”œâ”€â”€ 1_lexicon_generation/
â”‚   â”‚   # Marginal Screening plots and percentile threshold visualizations.
â”‚   â”œâ”€â”€ 2_feature_engineering/
â”‚   â”‚   # Embedding illustrations and vector representations.
â”‚   â”œâ”€â”€ 3_news_clustering/
â”‚   â”‚   # Silhouette scores, dendrograms, t-SNE visualizations.
â”‚   â”œâ”€â”€ 4_outlier_removal/
â”‚   â”‚   # Cluster cleaning and centroid repositioning figures.
â”‚   â”œâ”€â”€ 5_relevant_words_extraction/
â”‚   â”‚   # TF-IDF keyword extraction results per cluster.
â”‚   â”œâ”€â”€ 6_tweets_assignment/
â”‚   â”‚   # Tweet-to-cluster similarity visualizations.
â”‚   â”œâ”€â”€ 7_alert_generation/
â”‚   â”‚   # Alert ratio plots and ground truth comparisons.
â”‚   â”œâ”€â”€ 6_tweets_assignment.zip
â”‚   â”‚   # Archived visual results for tweet assignment.
â”‚   â””â”€â”€ 7_alert_generation.zip
â”‚       # Archived visual results for alert generation.
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 0_extract_data.ipynb
â”‚   â”‚   # Data ingestion, preprocessing, sentiment computation (VADER).
â”‚   â”œâ”€â”€ 1_lexicon_generation.ipynb
â”‚   â”‚   # Implementation of Marginal Screening and daily lexicon construction.
â”‚   â”œâ”€â”€ 2_feature_engineering.ipynb
â”‚   â”‚   # GloVe embedding computation and document vector construction.
â”‚   â”œâ”€â”€ 3_news_clustering.ipynb
â”‚   â”‚   # Hierarchical clustering (HAC), silhouette maximization, centroid computation.
â”‚   â”œâ”€â”€ 4_outlier_removal.ipynb
â”‚   â”‚   # Double-criterion outlier removal (silhouette + cosine similarity).
â”‚   â”œâ”€â”€ 5_relevant_words_extraction.ipynb
â”‚   â”‚   # TF-IDF keyword extraction per cluster.
â”‚   â”œâ”€â”€ 6_tweet_assignment.ipynb
â”‚   â”‚   # Tweet embedding and cosine similarity-based assignment.
â”‚   â””â”€â”€ 7_Alert_generation.ipynb
â”‚       # Alert computation, ground truth construction, Precision/Recall/F-score evaluation.
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   # Compiled Python bytecode (ignored by git).
â”‚   â”œâ”€â”€ alert_generation.py
â”‚   â”‚   # Computes daily assignment ratio R(d), generates alerts (R(d) > Î¸),
â”‚   â”‚   # builds S&P 500 ground truth (|weekly return| > 2%), and evaluates Precision/Recall/F-score.
â”‚   â”œâ”€â”€ extract_data.py
â”‚   â”‚   # Handles dataset loading, cleaning, date alignment,
â”‚   â”‚   # tweet filtering by cashtags, and sentiment scoring (VADER).
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   # Filters articles using daily lexicons and computes 300D GloVe embeddings.
â”‚   â”œâ”€â”€ lexicon_generation.py
â”‚   â”‚   # Builds daily binary DTM, computes Marginal Screening f(j),
â”‚   â”‚   # selects positive/negative financial terms via percentile thresholds.
â”‚   â”œâ”€â”€ news_clustering.py
â”‚   â”‚   # Runs Agglomerative Clustering (cosine + average linkage),
â”‚   â”‚   # compares with K-Means/K-Medians, computes silhouette scores,
â”‚   â”‚   # and calculates median-based centroids (event signatures).
â”‚   â”œâ”€â”€ outlier_removal.py
â”‚   â”‚   # Applies double filtering: per-sample silhouette + centroid cosine similarity.
â”‚   â”‚   # Removes noisy articles and recalculates centroids.
â”‚   â”œâ”€â”€ relevant_words_extraction.py
â”‚   â”‚   # Computes TF-IDF within each cluster to extract top representative financial terms.
â”‚   â””â”€â”€ tweet_assignment.py
â”‚       # Embeds tweets using the same GloVe model,
â”‚       # assigns them to nearest event centroids via cosine similarity (threshold = 0.5).
â”‚
â”œâ”€â”€ .gitignore
â”‚   # Excludes large data files, virtual environments, and cache folders.
â”‚
â”œâ”€â”€ .python-version
â”‚   # Specifies Python interpreter version for reproducibility.
â”‚
â”œâ”€â”€ LICENSE
â”‚   # Project license.
â”‚
â”œâ”€â”€ README.md
â”‚   # Project documentation and methodological overview.
â”‚
â”œâ”€â”€ pyproject.toml
â”‚   # Project metadata and dependency management (uv-compatible).
â”‚
â””â”€â”€ uv.lock
    # Locked dependency versions ensuring reproducible environments.
```

---

## Installation & Setup

We use uv, an extremely fast Python package and project manager written in Rust, to handle our virtual environment and dependencies.

### 1. Install uv
If you haven't installed uv yet, run:

```Bash
# On Windows (PowerShell)
pip install uv
# On macOS/Linux
curl -LsSf [https://astral.sh/uv/install.sh](https://astral.sh/uv/install.sh) | sh
```
### 2. Setup the Virtual Environment
Navigate to the project directory, create and activate the environment:

```Bash
cd Financial-Events-clustering-news-tweets
uv venv
# Windows:
 .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate
```
3. Install Dependencies
Install the required packages (Pandas, Numpy, Scikit-learn, Plotly, SciPy, Gensim):

```Bash
uv sync
```


# Financial Event Detection using News & Tweets (S&P 500 â€“ 2023)

## Abstract

This project reproduces and extends the methodology of Carta et al. (2021) for financial event detection using hierarchical clustering of news articles combined with social media resonance analysis.  

Applied to the S&P 500 over 2023, our pipeline integrates dynamic lexicon generation, document embeddings, hierarchical clustering, tweet assignment via cosine similarity, and alert generation based on social activity.  

The objective is to detect financially significant events in near real-time and evaluate performance against market ground truth derived from S&P 500 weekly variations.

---

# Methodology & Pipeline

Our pipeline follows the 7-step framework proposed by Carta et al. (2021), adapted to the 2023 S&P 500 context.

---

# Step 1 â€” Lexicon Generation

To reduce textual noise and retain financially meaningful signals, we construct a **dynamic domain-specific lexicon**.

- A **binary Document-Term Matrix (DTM)** is built over a rolling 4-week window.
- We compute the **Marginal Screening score** for each term:

\[
f(j) = \frac{1}{N} \sum_{k=1}^{N} X_k^{(j)} \cdot \delta_k
\]

Where:
- \( X_k^{(j)} \in \{0,1\} \) indicates the presence of term \( j \) in article \( k \)
- \( \delta_k \) is the daily S&P 500 return
- \( N \) is the number of articles in the window

- Terms above the 80th percentile (positive impact) and below the 20th percentile (negative impact) are retained.
- Neutral terms are discarded.

This produces a daily financial lexicon capturing market-relevant vocabulary.

ðŸ“Œ *Insert Screenshot:* Marginal Screening score distribution or lexicon word cloud.

---

# Step 2 â€” Feature Engineering (Embeddings)

Each news article is transformed into a dense numerical vector.

- Texts are tokenized and cleaned.
- Words not present in the daily lexicon are discarded.
- Pre-trained embeddings (GloVe 300D) are used.
- The document embedding is computed as:

\[
v_a = \frac{1}{|W_a|} \sum_{w \in W_a} \text{Embedding}(w)
\]

Where \( W_a \) is the set of lexicon-filtered words in article \( a \).

This step converts financial text into structured mathematical representations.

ðŸ“Œ *Insert Screenshot:* Example of 300D embeddings table or embedding visualization.

---

# Step 3 â€” News Clustering

We group news articles into candidate financial events.

Algorithms tested:
- K-Means  
- Agglomerative Clustering (HAC)  
- K-Medians  
- (Optional comparison: DBSCAN / GMM)

The optimal number of clusters \( k \) is selected by maximizing the **Silhouette Score**:

\[
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
\]

Where:
- \( a(i) \) = intra-cluster distance
- \( b(i) \) = nearest-cluster distance

Consistent with the original paper, **Hierarchical Agglomerative Clustering (cosine distance + average linkage)** produced the most coherent and interpretable clusters.

ðŸ“Œ *Insert Screenshot:* Silhouette maximization plot or t-SNE cluster visualization.

---

# Step 4 â€” Relevant Words Extraction & Outlier Removal

## Relevant Words Extraction

- We compute **Average TF-IDF** per cluster.
- The top representative financial terms are extracted.
- This improves cluster interpretability.

## Outlier Removal

We apply a **double filtering criterion**:

1. Per-sample Silhouette score  
2. Cosine similarity to cluster centroid  

Articles below the percentile threshold in either metric are removed.

Clusters lacking strong financial relevance are discarded.

ðŸ“Œ *Insert Screenshot:* Bar chart of key financial terms per cluster.

---

# Step 5 â€” Event Signatures

For each validated cluster, we compute a robust centroid:

\[
c_k = \text{median}(\{v_a : a \in \text{cluster}_k\})
\]

This centroid represents the **Event Signature**, acting as a mathematical summary of the event and serving as a reference for social media matching.

ðŸ“Œ *Insert Screenshot:* Cleaned clusters with centroid markers.

---

# Step 6 â€” Tweet Assignment

We measure public attention by linking tweets to event signatures.

Process:

- **De-duplication:** Remove identical tweets.
- **Embedding:** Tweets are embedded using the same 300D model.
- **Cosine Similarity:** Each tweet is compared to event centroids.

\[
\text{sim}(t, c_k) = \frac{t \cdot c_k}{\|t\| \|c_k\|}
\]

- Tweets with similarity â‰¥ threshold \( \delta \) are assigned to the event.

This quantifies social resonance around detected events.

ðŸ“Œ *Insert Screenshot:* Tweet similarity distribution or assignment plot.

---

# Step 7 â€” Alert Generation & Evaluation

## Social Heat

We define the daily assignment ratio:

\[
R(d) = \frac{\text{Assigned Tweets}_d}{\text{Total Tweets}_d}
\]

An alert is triggered when:

\[
R(d) > \theta
\]

---

## Ground Truth Construction

To evaluate performance, we define market event intervals using weekly S&P 500 variation:

\[
\Delta_d = \frac{|close(d+7) - close(d)|}{close(d)}
\]

Days satisfying:

\[
\Delta_d > 0.02
\]

are labeled as event days. Consecutive event days are aggregated into event intervals.

---

## Evaluation Metrics

We compute:

- **Precision**
- **Recall**
- **F-Score**

These metrics measure alignment between generated alerts and true market events.

ðŸ“Œ *Insert Screenshot:* Plot showing S&P 500 price, ground truth intervals, and social alerts.

---

# Case Studies (2023)

## Silicon Valley Bank Collapse (March 2023)

- Significant spike in Social Heat
- Clear cluster separation
- Detected prior to major market drawdown

## AI Boom & Nvidia Rally (Mayâ€“July 2023)

- Technology-focused clusters
- Strong resonance between news and tweets
- Captured momentum shift

## ARM IPO (September 2023)

- Anticipation reflected in clustering
- Immediate post-listing social amplification

---

# Key Findings

## HAC Dominance

Hierarchical Agglomerative Clustering consistently outperformed K-Means and density-based approaches in producing semantically coherent clusters.

## Recall over Precision

In financial risk management, missing a crash (low Recall) is more costly than issuing a false alert (low Precision).  
Our model prioritizes Recall and successfully captures most ground truth events.

## Social Hype Dynamics

- News dissemination is immediate.
- Social Heat may anticipate or slightly lag price reactions.
- This temporal asymmetry may provide exploitable alpha signals.

---

# Acknowledgments

**Original Authors:**  
Carta, S., et al. (2021). *Event Detection in Finance by Clustering News and Tweets.*

**Institution:**  
UniversitÃ© Paris 1 PanthÃ©on-Sorbonne â€” Master 2 MOSEF (Quantitative Finance)
