# Event Detection in Finance by Clustering News and Tweets ðŸ“ˆðŸ“°

![Python Badge](https://img.shields.io/badge/Python-3.10%2B-blue)
![Data Science Badge](https://img.shields.io/badge/Data_Science-NLP_%7C_Clustering-orange)
![Academic Badge](https://img.shields.io/badge/UniversitÃ©_Paris_1-PanthÃ©on_Sorbonne-maroon)

**Authors:** Roland DUTAUZIET & Maeva N'GUESSAN
**Program:** Master 2 MOSEF - Data Science (ModÃ©lisations Statistiques Ã‰conomiques et FinanciÃ¨res) - 2025/2026  
**Context:** Quantitative Finance Project

## ðŸ“– Project Overview

This project is a reproduction and an extension of the academic paper *"Event Detection in Finance by Clustering News and Tweets"* (Carta et al., 2021). The core objective is to build a robust NLP pipeline capable of detecting major financial events by clustering professional news articles, and then validating these events by measuring the "Social Heat" (public attention) through social media platforms (Twitter/Stocktwits).

We applied this methodology to the **S&P 500 index for the year 2023**, a period marked by high volatility, including the Silicon Valley Bank (SVB) collapse and and the ARM IPO.



---

## ðŸ—ï¸ Project Architecture

The repository is structured to ensure reproducibility and clean code separation.

```text
Financial-Events-clustering-news-tweets
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ for_models/
â”‚   â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”‚   â”œâ”€â”€ table_3_tweet_assignment_AI.csv
â”‚   â”‚   â”‚   â”‚   # Tweet-to-cluster assignment results for the AI enthusiasm period (Mayâ€“July 2023).
â”‚   â”‚   â”‚   â”‚   # Used for quantitative evaluation and representative tweet analysis.
â”‚   â”‚   â”‚   â”œâ”€â”€ table_3_tweet_assignment_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Tweet assignment results for the Silicon Valley Bank crisis period (March 2023).
â”‚   â”‚   â”‚   â”‚   # Contains cosine similarity scores and assigned event IDs.
â”‚   â”‚   â”‚   â”œâ”€â”€ clean_news_week_AI.csv
â”‚   â”‚   â”‚   â”‚   # Preprocessed weekly news dataset for the AI event window.
â”‚   â”‚   â”‚   â”œâ”€â”€ clean_news_week_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Preprocessed weekly news dataset for the SVB crisis window.
â”‚   â”‚   â”‚   â”œâ”€â”€ final_event_signatures_AI.csv
â”‚   â”‚   â”‚   â”‚   # Median-based centroids (event signatures) after outlier removal (AI period).
â”‚   â”‚   â”‚   â”œâ”€â”€ final_event_signatures_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Robust cluster centroids after cleaning (SVB period).
â”‚   â”‚   â”‚   â”œâ”€â”€ news_features.csv
â”‚   â”‚   â”‚   â”‚   # 300-dimensional document embeddings (GloVe) for each news article.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned.csv
â”‚   â”‚   â”‚   â”‚   # Full tweet assignment output across all periods.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned_AI.csv
â”‚   â”‚   â”‚   â”‚   # Tweets assigned to clusters during AI enthusiasm period.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Tweets assigned to clusters during SVB crisis.
â”‚   â”‚   â”‚   â””â”€â”€ tweets_features.csv
â”‚   â”‚   â”‚       # 300-dimensional embeddings for tweets (same GloVe model as news).
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”‚       â”œâ”€â”€ daily_dtm/
â”‚   â”‚       â”‚   # Daily binary Document-Term Matrices used for Marginal Screening.
â”‚   â”‚       â”œâ”€â”€ daily_lexicons_filtered/
â”‚   â”‚       â”‚   # Daily filtered lexicons (P20/P80 percentile selection).
â”‚   â”‚       â”œâ”€â”€ daily_lexicons_full/
â”‚   â”‚       â”‚   # Full lexicons before percentile filtering.
â”‚   â”‚       â”œâ”€â”€ news_2023.csv
â”‚   â”‚       â”‚   # 1,565 financial news articles (GDELT, Yahoo Finance, CNBC).
â”‚   â”‚       â”œâ”€â”€ news_2023_clean.csv
â”‚   â”‚       â”‚   # Cleaned and preprocessed version of news_2023.csv.
â”‚   â”‚       â”œâ”€â”€ sp500_2023.csv
â”‚   â”‚       â”‚   # Daily S&P 500 prices (271 trading days). Used for return computation and ground truth.
â”‚   â”‚       â””â”€â”€ tweets_2023.csv
â”‚   â”‚           # 2,243 filtered financial tweets mentioning $SPX, $SPY, or S&P 500.
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ P10 Event detection in finance using ...
â”‚   â”‚   # Original reference paper (Carta et al., 2021).
â”‚   â”œâ”€â”€ Rapport_Event_detection_Roland_...
â”‚   â”‚   # Full academic report (methodology, results, evaluation).
â”‚   â””â”€â”€ Slides Finance quantitative.pdf
â”‚       # Presentation slides summarizing the project.
â”‚
â”œâ”€â”€ img/
â”‚   â”œâ”€â”€ 1_lexicon_generation/
â”‚   â”‚   # Marginal Screening plots and percentile threshold visualizations.
â”‚   â”œâ”€â”€ 2_feature_engineering/
â”‚   â”‚   # Embedding illustrations and vector representations.
â”‚   â”œâ”€â”€ 3_news_clustering/
â”‚   â”‚   # Silhouette scores, dendrograms, t-SNE visualizations.
â”‚   â”œâ”€â”€ 4_outlier_removal/
â”‚   â”‚   # Cluster cleaning and centroid repositioning figures.
â”‚   â”œâ”€â”€ 5_relevant_words_extraction/
â”‚   â”‚   # TF-IDF keyword extraction results per cluster.
â”‚   â”œâ”€â”€ 6_tweets_assignment/
â”‚   â”‚   # Tweet-to-cluster similarity visualizations.
â”‚   â”œâ”€â”€ 7_alert_generation/
â”‚   â”‚   # Alert ratio plots and ground truth comparisons.
â”‚   â”œâ”€â”€ 6_tweets_assignment.zip
â”‚   â”‚   # Archived visual results for tweet assignment.
â”‚   â””â”€â”€ 7_alert_generation.zip
â”‚       # Archived visual results for alert generation.
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 0_extract_data.ipynb
â”‚   â”‚   # Data ingestion, preprocessing, sentiment computation (VADER).
â”‚   â”œâ”€â”€ 1_lexicon_generation.ipynb
â”‚   â”‚   # Implementation of Marginal Screening and daily lexicon construction.
â”‚   â”œâ”€â”€ 2_feature_engineering.ipynb
â”‚   â”‚   # GloVe embedding computation and document vector construction.
â”‚   â”œâ”€â”€ 3_news_clustering.ipynb
â”‚   â”‚   # Hierarchical clustering (HAC), silhouette maximization, centroid computation.
â”‚   â”œâ”€â”€ 4_outlier_removal.ipynb
â”‚   â”‚   # Double-criterion outlier removal (silhouette + cosine similarity).
â”‚   â”œâ”€â”€ 5_relevant_words_extraction.ipynb
â”‚   â”‚   # TF-IDF keyword extraction per cluster.
â”‚   â”œâ”€â”€ 6_tweet_assignment.ipynb
â”‚   â”‚   # Tweet embedding and cosine similarity-based assignment.
â”‚   â””â”€â”€ 7_Alert_generation.ipynb
â”‚       # Alert computation, ground truth construction, Precision/Recall/F-score evaluation.
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   # Compiled Python bytecode (ignored by git).
â”‚   â”œâ”€â”€ alert_generation.py
â”‚   â”‚   # Computes daily assignment ratio R(d), generates alerts (R(d) > Î¸),
â”‚   â”‚   # builds S&P 500 ground truth (|weekly return| > 2%), and evaluates Precision/Recall/F-score.
â”‚   â”œâ”€â”€ extract_data.py
â”‚   â”‚   # Handles dataset loading, cleaning, date alignment,
â”‚   â”‚   # tweet filtering by cashtags, and sentiment scoring (VADER).
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   # Filters articles using daily lexicons and computes 300D GloVe embeddings.
â”‚   â”œâ”€â”€ lexicon_generation.py
â”‚   â”‚   # Builds daily binary DTM, computes Marginal Screening f(j),
â”‚   â”‚   # selects positive/negative financial terms via percentile thresholds.
â”‚   â”œâ”€â”€ news_clustering.py
â”‚   â”‚   # Runs Agglomerative Clustering (cosine + average linkage),
â”‚   â”‚   # compares with K-Means/K-Medians, computes silhouette scores,
â”‚   â”‚   # and calculates median-based centroids (event signatures).
â”‚   â”œâ”€â”€ outlier_removal.py
â”‚   â”‚   # Applies double filtering: per-sample silhouette + centroid cosine similarity.
â”‚   â”‚   # Removes noisy articles and recalculates centroids.
â”‚   â”œâ”€â”€ relevant_words_extraction.py
â”‚   â”‚   # Computes TF-IDF within each cluster to extract top representative financial terms.
â”‚   â””â”€â”€ tweet_assignment.py
â”‚       # Embeds tweets using the same GloVe model,
â”‚       # assigns them to nearest event centroids via cosine similarity (threshold = 0.5).
â”‚
â”œâ”€â”€ .gitignore
â”‚   # Excludes large data files, virtual environments, and cache folders.
â”‚
â”œâ”€â”€ .python-version
â”‚   # Specifies Python interpreter version for reproducibility.
â”‚
â”œâ”€â”€ LICENSE
â”‚   # Project license.
â”‚
â”œâ”€â”€ README.md
â”‚   # Project documentation and methodological overview.
â”‚
â”œâ”€â”€ pyproject.toml
â”‚   # Project metadata and dependency management (uv-compatible).
â”‚
â””â”€â”€ uv.lock
    # Locked dependency versions ensuring reproducible environments.
```

---

## Installation & Setup

We use uv, an extremely fast Python package and project manager written in Rust, to handle our virtual environment and dependencies.

### 1. Install uv
If you haven't installed uv yet, run:

```Bash
# On Windows (PowerShell)
pip install uv
# On macOS/Linux
curl -LsSf [https://astral.sh/uv/install.sh](https://astral.sh/uv/install.sh) | sh
```
### 2. Setup the Virtual Environment
Navigate to the project directory, create and activate the environment:

```Bash
cd Financial-Events-clustering-news-tweets
uv venv
# Windows:
 .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate
```
3. Install Dependencies
Install the required packages (Pandas, Numpy, Scikit-learn, Plotly, SciPy, Gensim):

```Bash
uv sync
```
Methodology & Pipeline

Our pipeline reproduces the 7-step framework proposed by Carta et al. (2021), adapted to the 2023 S&P 500 context.
Each phase is executed sequentially to detect financially significant events from news and social media data.

Step 1 â€” Lexicon Generation

To reduce textual noise and retain only financially meaningful signals, we construct a dynamic domain-specific lexicon.

A binary Document-Term Matrix (DTM) is built over a rolling 4-week window.

We apply Marginal Screening to compute a score 
ð‘“
(
ð‘—
)
f(j) for each term:

ð‘“
(
ð‘—
)
=
1
ð‘
âˆ‘
ð‘˜
=
1
ð‘
ð‘‹
ð‘˜
(
ð‘—
)
â‹…
ð›¿
ð‘˜
f(j)=
N
1
	â€‹

k=1
âˆ‘
N
	â€‹

X
k
(j)
	â€‹

â‹…Î´
k
	â€‹


Terms above the 80th percentile (positive impact) and below the 20th percentile (negative impact) are retained.

Neutral words are discarded.

This produces a daily financial lexicon capturing market-relevant vocabulary.

ðŸ“Œ Insert Screenshot: Lexicon distribution plot or Marginal Screening score chart.

Step 2 â€” Feature Engineering (Embeddings)

Each news article is transformed into a dense numerical representation.

Texts are tokenized and cleaned.

Words not present in the daily lexicon are discarded.

Pre-trained word embeddings are used (GloVe 300D in our implementation).

The document embedding is computed as the average of valid word vectors:

ð‘£
ð‘Ž
=
1
âˆ£
ð‘Š
ð‘Ž
âˆ£
âˆ‘
ð‘¤
âˆˆ
ð‘Š
ð‘Ž
Embedding
(
ð‘¤
)
v
a
	â€‹

=
âˆ£W
a
	â€‹

âˆ£
1
	â€‹

wâˆˆW
a
	â€‹

âˆ‘
	â€‹

Embedding(w)

This converts raw financial text into structured mathematical vectors suitable for clustering.

ðŸ“Œ Insert Screenshot: Example of 300D embeddings table or embedding visualization.

Step 3 â€” News Clustering

We group news articles into candidate financial events.

Algorithms tested:

K-Means

Agglomerative Clustering (HAC)

K-Medians

(Optional comparison: DBSCAN / GMM depending on experiment)

The number of clusters 
ð‘˜
k is optimized via Silhouette Score maximization:

ð‘ 
(
ð‘–
)
=
ð‘
(
ð‘–
)
âˆ’
ð‘Ž
(
ð‘–
)
max
â¡
(
ð‘Ž
(
ð‘–
)
,
ð‘
(
ð‘–
)
)
s(i)=
max(a(i),b(i))
b(i)âˆ’a(i)
	â€‹


Consistent with the original paper, Hierarchical Agglomerative Clustering (cosine distance + average linkage) achieved the best performance, producing compact and semantically coherent clusters.

ðŸ“Œ Insert Screenshot: Silhouette Score comparison or t-SNE cluster visualization.

Step 4 â€” Relevant Words Extraction & Outlier Removal

Not all clusters represent true financial events.

Relevant Words Extraction

We compute Average TF-IDF per cluster.

The top representative financial terms are extracted.

This provides interpretability for each detected event.

Outlier Removal

We apply a double filtering criterion:

Per-sample Silhouette score

Cosine similarity to cluster centroid

Articles below the percentile threshold in either metric are removed.

Clusters lacking strong financial relevance are discarded.

ðŸ“Œ Insert Screenshot: Bar chart of top financial keywords per cluster.

Step 5 â€” Event Signatures

For validated clusters, we compute a robust centroid:

ð‘
ð‘˜
=
median
(
{
ð‘£
ð‘Ž
:
ð‘Ž
âˆˆ
cluster
ð‘˜
}
)
c
k
	â€‹

=median({v
a
	â€‹

:aâˆˆcluster
k
	â€‹

})

This centroid represents the Event Signature â€” a compact mathematical summary of the event.

It acts as a gravitational anchor for social media resonance detection.

ðŸ“Œ Insert Screenshot: Cleaned clusters with centroid markers.

Step 6 â€” Tweet Assignment

We measure public attention by linking tweets to event signatures.

Process:

De-duplication: Remove identical tweets (anti-spam).

Embedding: Tweets are embedded using the same 300D model.

Cosine Similarity: Each tweet is compared to event centroids.

Threshold 
ð›¿
Î´: Tweets with similarity â‰¥ threshold are assigned to the event.

sim
(
ð‘¡
,
ð‘
ð‘˜
)
=
ð‘¡
â‹…
ð‘
ð‘˜
âˆ¥
ð‘¡
âˆ¥
âˆ¥
ð‘
ð‘˜
âˆ¥
sim(t,c
k
	â€‹

)=
âˆ¥tâˆ¥âˆ¥c
k
	â€‹

âˆ¥
tâ‹…c
k
	â€‹

	â€‹


This step quantifies social resonance around detected events.

ðŸ“Œ Insert Screenshot: Tweet similarity distribution or assignment visualization.

Step 7 â€” Alert Generation & Evaluation
Alert Generation

We define Social Heat:

ð‘…
(
ð‘‘
)
=
Assigned Tweets
ð‘‘
Total Tweets
ð‘‘
R(d)=
Total Tweets
d
	â€‹

Assigned Tweets
d
	â€‹

	â€‹


An alert is triggered if:

ð‘…
(
ð‘‘
)
>
ðœƒ
R(d)>Î¸
Ground Truth Construction

To evaluate performance, we define market event intervals based on weekly S&P 500 variation:

Î”
ð‘‘
=
âˆ£
ð‘
ð‘™
ð‘œ
ð‘ 
ð‘’
(
ð‘‘
+
7
)
âˆ’
ð‘
ð‘™
ð‘œ
ð‘ 
ð‘’
(
ð‘‘
)
âˆ£
ð‘
ð‘™
ð‘œ
ð‘ 
ð‘’
(
ð‘‘
)
Î”
d
	â€‹

=
close(d)
âˆ£close(d+7)âˆ’close(d)âˆ£
	â€‹


Days where:

Î”
ð‘‘
>
0.02
Î”
d
	â€‹

>0.02

are labeled as event days, and consecutive event days are aggregated into intervals.

Evaluation Metrics

We compute:

Precision

Recall

F-Score

These metrics measure the alignment between generated alerts and true market events.

ðŸ“Œ Insert Screenshot: Plotly chart showing S&P 500 price, ground truth intervals, and social alerts.

Case Studies (2023)

Our pipeline successfully detected major market events:

Silicon Valley Bank Collapse (March 2023)

Sharp spike in Social Heat

Strong clustering structure

Detected before major market drawdown

AI Boom & Nvidia Rally (Mayâ€“July 2023)

Technology-related clusters

Strong resonance between news and tweets

Captured market momentum shift

ARM IPO (September 2023)

Anticipation reflected in clustering

Immediate post-listing social reaction

Key Findings
HAC Dominance

Hierarchical Agglomerative Clustering consistently outperformed K-Means and other methods in generating semantically coherent clusters.

Recall over Precision

In a trading context, missing a crash (low Recall) is more costly than a false alert (low Precision).
Our model prioritizes Recall and successfully captures most ground truth events.

Social Hype Dynamics

The pipeline shows that:

News breaks instantly.

Social Heat sometimes anticipates or slightly lags price impact.

This temporal asymmetry may provide exploitable alpha signals.

Acknowledgments

Original Authors:
Carta, S., et al. (2021). Event Detection in Finance by Clustering News and Tweets.

Institution:
UniversitÃ© Paris 1 PanthÃ©on-Sorbonne â€” Master 2 MOSEF (Quantitative Finance)
