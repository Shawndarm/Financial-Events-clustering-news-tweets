# Event Detection in Finance by Clustering News and Tweets ðŸ“ˆðŸ“°

![Python Badge](https://img.shields.io/badge/Python-3.10%2B-blue)
![Data Science Badge](https://img.shields.io/badge/Data_Science-NLP_%7C_Clustering-orange)
![Academic Badge](https://img.shields.io/badge/UniversitÃ©_Paris_1-PanthÃ©on_Sorbonne-maroon)

**Authors:** Roland DUTAUZIET & Maeva N'GUESSAN
**Program:** Master 2 MOSEF - Data Science (ModÃ©lisations Statistiques Ã‰conomiques et FinanciÃ¨res) - 2025/2026  
**Context:** Quantitative Finance Project

## ðŸ“– Project Overview

This project is a reproduction and an extension of the academic paper *"Event Detection in Finance by Clustering News and Tweets"* (Carta et al., 2021). The core objective is to build a robust NLP pipeline capable of detecting major financial events by clustering professional news articles, and then validating these events by measuring the "Social Heat" (public attention) through social media platforms (Twitter/Stocktwits).

We applied this methodology to the **S&P 500 index for the year 2023**, a period marked by high volatility, including the Silicon Valley Bank (SVB) collapse and and the ARM IPO.



---

## ðŸ—ï¸ Project Architecture

The repository is structured to ensure reproducibility and clean code separation.

```text
Financial-Events-clustering-news-tweets
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ for_models/
â”‚   â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”‚   â”œâ”€â”€ table_3_tweet_assignment_AI.csv
â”‚   â”‚   â”‚   â”‚   # Tweet-to-cluster assignment results for the AI enthusiasm period (Mayâ€“July 2023).
â”‚   â”‚   â”‚   â”‚   # Used for quantitative evaluation and representative tweet analysis.
â”‚   â”‚   â”‚   â”œâ”€â”€ table_3_tweet_assignment_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Tweet assignment results for the Silicon Valley Bank crisis period (March 2023).
â”‚   â”‚   â”‚   â”‚   # Contains cosine similarity scores and assigned event IDs.
â”‚   â”‚   â”‚   â”œâ”€â”€ clean_news_week_AI.csv
â”‚   â”‚   â”‚   â”‚   # Preprocessed weekly news dataset for the AI event window.
â”‚   â”‚   â”‚   â”œâ”€â”€ clean_news_week_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Preprocessed weekly news dataset for the SVB crisis window.
â”‚   â”‚   â”‚   â”œâ”€â”€ final_event_signatures_AI.csv
â”‚   â”‚   â”‚   â”‚   # Median-based centroids (event signatures) after outlier removal (AI period).
â”‚   â”‚   â”‚   â”œâ”€â”€ final_event_signatures_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Robust cluster centroids after cleaning (SVB period).
â”‚   â”‚   â”‚   â”œâ”€â”€ news_features.csv
â”‚   â”‚   â”‚   â”‚   # 300-dimensional document embeddings (GloVe) for each news article.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned.csv
â”‚   â”‚   â”‚   â”‚   # Full tweet assignment output across all periods.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned_AI.csv
â”‚   â”‚   â”‚   â”‚   # Tweets assigned to clusters during AI enthusiasm period.
â”‚   â”‚   â”‚   â”œâ”€â”€ tweets_assigned_SVB.csv
â”‚   â”‚   â”‚   â”‚   # Tweets assigned to clusters during SVB crisis.
â”‚   â”‚   â”‚   â””â”€â”€ tweets_features.csv
â”‚   â”‚   â”‚       # 300-dimensional embeddings for tweets (same GloVe model as news).
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”‚       â”œâ”€â”€ daily_dtm/
â”‚   â”‚       â”‚   # Daily binary Document-Term Matrices used for Marginal Screening.
â”‚   â”‚       â”œâ”€â”€ daily_lexicons_filtered/
â”‚   â”‚       â”‚   # Daily filtered lexicons (P20/P80 percentile selection).
â”‚   â”‚       â”œâ”€â”€ daily_lexicons_full/
â”‚   â”‚       â”‚   # Full lexicons before percentile filtering.
â”‚   â”‚       â”œâ”€â”€ news_2023.csv
â”‚   â”‚       â”‚   # 1,565 financial news articles (GDELT, Yahoo Finance, CNBC).
â”‚   â”‚       â”œâ”€â”€ news_2023_clean.csv
â”‚   â”‚       â”‚   # Cleaned and preprocessed version of news_2023.csv.
â”‚   â”‚       â”œâ”€â”€ sp500_2023.csv
â”‚   â”‚       â”‚   # Daily S&P 500 prices (271 trading days). Used for return computation and ground truth.
â”‚   â”‚       â””â”€â”€ tweets_2023.csv
â”‚   â”‚           # 2,243 filtered financial tweets mentioning $SPX, $SPY, or S&P 500.
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ P10 Event detection in finance using ...
â”‚   â”‚   # Original reference paper (Carta et al., 2021).
â”‚   â”œâ”€â”€ Rapport_Event_detection_Roland_...
â”‚   â”‚   # Full academic report (methodology, results, evaluation).
â”‚   â””â”€â”€ Slides Finance quantitative.pdf
â”‚       # Presentation slides summarizing the project.
â”‚
â”œâ”€â”€ img/
â”‚   â”œâ”€â”€ 1_lexicon_generation/
â”‚   â”‚   # Marginal Screening plots and percentile threshold visualizations.
â”‚   â”œâ”€â”€ 2_feature_engineering/
â”‚   â”‚   # Embedding illustrations and vector representations.
â”‚   â”œâ”€â”€ 3_news_clustering/
â”‚   â”‚   # Silhouette scores, dendrograms, t-SNE visualizations.
â”‚   â”œâ”€â”€ 4_outlier_removal/
â”‚   â”‚   # Cluster cleaning and centroid repositioning figures.
â”‚   â”œâ”€â”€ 5_relevant_words_extraction/
â”‚   â”‚   # TF-IDF keyword extraction results per cluster.
â”‚   â”œâ”€â”€ 6_tweets_assignment/
â”‚   â”‚   # Tweet-to-cluster similarity visualizations.
â”‚   â”œâ”€â”€ 7_alert_generation/
â”‚   â”‚   # Alert ratio plots and ground truth comparisons.
â”‚   â”œâ”€â”€ 6_tweets_assignment.zip
â”‚   â”‚   # Archived visual results for tweet assignment.
â”‚   â””â”€â”€ 7_alert_generation.zip
â”‚       # Archived visual results for alert generation.
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 0_extract_data.ipynb
â”‚   â”‚   # Data ingestion, preprocessing, sentiment computation (VADER).
â”‚   â”œâ”€â”€ 1_lexicon_generation.ipynb
â”‚   â”‚   # Implementation of Marginal Screening and daily lexicon construction.
â”‚   â”œâ”€â”€ 2_feature_engineering.ipynb
â”‚   â”‚   # GloVe embedding computation and document vector construction.
â”‚   â”œâ”€â”€ 3_news_clustering.ipynb
â”‚   â”‚   # Hierarchical clustering (HAC), silhouette maximization, centroid computation.
â”‚   â”œâ”€â”€ 4_outlier_removal.ipynb
â”‚   â”‚   # Double-criterion outlier removal (silhouette + cosine similarity).
â”‚   â”œâ”€â”€ 5_relevant_words_extraction.ipynb
â”‚   â”‚   # TF-IDF keyword extraction per cluster.
â”‚   â”œâ”€â”€ 6_tweet_assignment.ipynb
â”‚   â”‚   # Tweet embedding and cosine similarity-based assignment.
â”‚   â””â”€â”€ 7_Alert_generation.ipynb
â”‚       # Alert computation, ground truth construction, Precision/Recall/F-score evaluation.
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   # Compiled Python bytecode (ignored by git).
â”‚   â”œâ”€â”€ alert_generation.py
â”‚   â”‚   # Computes daily assignment ratio R(d), generates alerts (R(d) > Î¸),
â”‚   â”‚   # builds S&P 500 ground truth (|weekly return| > 2%), and evaluates Precision/Recall/F-score.
â”‚   â”œâ”€â”€ extract_data.py
â”‚   â”‚   # Handles dataset loading, cleaning, date alignment,
â”‚   â”‚   # tweet filtering by cashtags, and sentiment scoring (VADER).
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   # Filters articles using daily lexicons and computes 300D GloVe embeddings.
â”‚   â”œâ”€â”€ lexicon_generation.py
â”‚   â”‚   # Builds daily binary DTM, computes Marginal Screening f(j),
â”‚   â”‚   # selects positive/negative financial terms via percentile thresholds.
â”‚   â”œâ”€â”€ news_clustering.py
â”‚   â”‚   # Runs Agglomerative Clustering (cosine + average linkage),
â”‚   â”‚   # compares with K-Means/K-Medians, computes silhouette scores,
â”‚   â”‚   # and calculates median-based centroids (event signatures).
â”‚   â”œâ”€â”€ outlier_removal.py
â”‚   â”‚   # Applies double filtering: per-sample silhouette + centroid cosine similarity.
â”‚   â”‚   # Removes noisy articles and recalculates centroids.
â”‚   â”œâ”€â”€ relevant_words_extraction.py
â”‚   â”‚   # Computes TF-IDF within each cluster to extract top representative financial terms.
â”‚   â””â”€â”€ tweet_assignment.py
â”‚       # Embeds tweets using the same GloVe model,
â”‚       # assigns them to nearest event centroids via cosine similarity (threshold = 0.5).
â”‚
â”œâ”€â”€ .gitignore
â”‚   # Excludes large data files, virtual environments, and cache folders.
â”‚
â”œâ”€â”€ .python-version
â”‚   # Specifies Python interpreter version for reproducibility.
â”‚
â”œâ”€â”€ LICENSE
â”‚   # Project license.
â”‚
â”œâ”€â”€ README.md
â”‚   # Project documentation and methodological overview.
â”‚
â”œâ”€â”€ pyproject.toml
â”‚   # Project metadata and dependency management (uv-compatible).
â”‚
â””â”€â”€ uv.lock
    # Locked dependency versions ensuring reproducible environments.
```

---

## Installation & Setup

We use uv, an extremely fast Python package and project manager written in Rust, to handle our virtual environment and dependencies.

### 1. Install uv
If you haven't installed uv yet, run:

```Bash
# On Windows (PowerShell)
pip install uv
# On macOS/Linux
curl -LsSf [https://astral.sh/uv/install.sh](https://astral.sh/uv/install.sh) | sh
```
### 2. Setup the Virtual Environment
Navigate to the project directory, create and activate the environment:

```Bash
cd Financial-Events-clustering-news-tweets
uv venv
# Windows:
 .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate
```
3. Install Dependencies
Install the required packages (Pandas, Numpy, Scikit-learn, Plotly, SciPy, Gensim):

```Bash
uv sync -- lock
```
